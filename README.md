This GitHub contains the finetuning of open source LLM like Falcon-7b, LLAMA2-7B, Mistral-7B, FLAN-T5, and CodeLlama-7b.

The techniques used are Gradient-based methods, Peft-Lora, Q-Lora, and ULMfit finetuning, which utilized various techniques to reduce the context length for improved inference time and employed Flash Attention and other methods for improved inference latency.
